{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data to fit DuETT's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from multiprocessing import Manager\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DialysisDataset(Dataset):\n",
    "    def __init__(self, split_name, data_path=None, n_timesteps=32, use_temp_cache=False, \n",
    "                 target_col='outcome', seed=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Dataset for dialysis lab data\n",
    "        \n",
    "        Args:\n",
    "            split_name: 'train', 'val', or 'test'\n",
    "            data_path: path to the pickle file with the analytics data\n",
    "            n_timesteps: Number of time bins to use for discretization\n",
    "            use_temp_cache: Whether to use a temporary cache for processed items\n",
    "            target_col: Column name for the target variable\n",
    "            seed: Random seed for train/val/test splits\n",
    "        \"\"\"\n",
    "        self.split_name = split_name\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.temp_cache = Manager().dict() if use_temp_cache else None\n",
    "        self.data_path = data_path\n",
    "        self.target_col = target_col\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Lab values to use\n",
    "        self.lab_columns = [\n",
    "            'GLUCOSA', 'UREA', 'CREATININA', 'URICO', 'SODIO', 'POTASIO', \n",
    "            'CALCIO', 'FOSFORO', 'HIERRO', 'TRANSFERRINA', 'IST', 'FERRITINA', \n",
    "            'COLESTEROL', 'TRIGLICERIDOS', 'HDL', 'LDL', 'PROTEINAS', 'ALBUMINA'\n",
    "        ]\n",
    "        \n",
    "        # Will be populated in setup()\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.patient_ids = None\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.maxes = []\n",
    "        self.mins = []\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Load and preprocess the data\"\"\"\n",
    "        # Load analytics data\n",
    "        analiticas = pd.read_pickle(self.data_path)\n",
    "        \n",
    "        # Convert date to datetime if not already\n",
    "        analiticas['FECHA'] = pd.to_datetime(analiticas['FECHA'])\n",
    "        \n",
    "        # Sort by patient ID and date\n",
    "        analiticas = analiticas.sort_values(['REGISTRO', 'FECHA'])\n",
    "        \n",
    "        # For this example, we'll use a simple dummy target - you should replace this\n",
    "        # with your actual target variable\n",
    "        if self.target_col not in analiticas.columns:\n",
    "            print(f\"Warning: Target column {self.target_col} not found. Creating dummy target.\")\n",
    "            # Create a dummy binary outcome (replace with actual outcome)\n",
    "            analiticas['outcome'] = (analiticas['ALBUMINA'] < analiticas['ALBUMINA'].median()).astype(int)\n",
    "        \n",
    "        # Get unique patients\n",
    "        patient_ids = analiticas['REGISTRO'].unique()\n",
    "        \n",
    "        # Create train/val/test splits by patient ID to avoid data leakage\n",
    "        train_ids, test_val_ids = train_test_split(\n",
    "            patient_ids, test_size=0.3, random_state=self.seed\n",
    "        )\n",
    "        val_ids, test_ids = train_test_split(\n",
    "            test_val_ids, test_size=0.5, random_state=self.seed\n",
    "        )\n",
    "        \n",
    "        # Select the appropriate patient IDs based on split\n",
    "        if self.split_name == 'train':\n",
    "            selected_ids = train_ids\n",
    "        elif self.split_name == 'val':\n",
    "            selected_ids = val_ids\n",
    "        elif self.split_name == 'test':\n",
    "            selected_ids = test_ids\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split name: {self.split_name}\")\n",
    "            \n",
    "        # Filter data for selected patients\n",
    "        split_data = analiticas[analiticas['REGISTRO'].isin(selected_ids)]\n",
    "        \n",
    "        # Create patient-level data structures\n",
    "        patients_data = []\n",
    "        patients_outcomes = []\n",
    "        \n",
    "        for patient_id in selected_ids:\n",
    "            patient_df = analiticas[analiticas['REGISTRO'] == patient_id]\n",
    "            \n",
    "            # Skip patients with less than 2 measurements\n",
    "            if len(patient_df) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Get lab values\n",
    "            labs = patient_df[self.lab_columns].values\n",
    "            \n",
    "            # Calculate time in days from first measurement\n",
    "            start_date = patient_df['FECHA'].min()\n",
    "            days_since_start = (patient_df['FECHA'] - start_date).dt.total_seconds() / (24 * 3600)\n",
    "            \n",
    "            # Stack time and labs\n",
    "            patient_data = np.column_stack([days_since_start.values, labs])\n",
    "            \n",
    "            # Get outcome (use the most recent outcome for the patient)\n",
    "            outcome = patient_df[self.target_col].iloc[-1]\n",
    "            \n",
    "            patients_data.append(patient_data)\n",
    "            patients_outcomes.append(outcome)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.X = [torch.tensor(data, dtype=torch.float32) for data in patients_data]\n",
    "        self.y = torch.tensor(patients_outcomes, dtype=torch.float32)\n",
    "        self.patient_ids = selected_ids\n",
    "        \n",
    "        # Calculate statistics for normalization\n",
    "        all_values = np.vstack([data for data in patients_data])\n",
    "        \n",
    "        for i in range(all_values.shape[1]):\n",
    "            column = all_values[:, i]\n",
    "            valid_values = column[~np.isnan(column)]\n",
    "            \n",
    "            if len(valid_values) > 0:\n",
    "                self.means.append(float(np.nanmean(valid_values)))\n",
    "                self.stds.append(float(np.nanstd(valid_values) or 1.0))  # Avoid division by zero\n",
    "                self.maxes.append(float(np.nanmax(valid_values)))\n",
    "                self.mins.append(float(np.nanmin(valid_values)))\n",
    "            else:\n",
    "                self.means.append(0.0)\n",
    "                self.stds.append(1.0)\n",
    "                self.maxes.append(0.0)\n",
    "                self.mins.append(0.0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Get a single patient's data, processed into fixed-length time bins\"\"\"\n",
    "        if self.temp_cache is not None and i in self.temp_cache:\n",
    "            return self.temp_cache[i]\n",
    "        \n",
    "        # Get data for this patient\n",
    "        ins = self.X[i]\n",
    "        \n",
    "        # Time is the first column\n",
    "        time = ins[:, 0]\n",
    "        \n",
    "        # No static features in this implementation, but we'll keep the structure\n",
    "        # for compatibility with DuETT\n",
    "        x_static = torch.zeros(self.d_static_num())\n",
    "        \n",
    "        # Create time series tensor with dimensions [n_timesteps, features*2]\n",
    "        # The second half of features is used as a binary mask for valid values\n",
    "        x_ts = torch.zeros((self.n_timesteps, self.d_time_series_num()*2))\n",
    "        \n",
    "        # Last time point (max days)\n",
    "        max_days = time[-1]\n",
    "        \n",
    "        # Process each time point\n",
    "        for i_t, t in enumerate(time):\n",
    "            # Determine which bin this time point belongs to\n",
    "            bin_idx = self.n_timesteps - 1 if t == max_days else int(t / max_days * self.n_timesteps)\n",
    "            \n",
    "            # Process each lab value at this time point\n",
    "            for i_lab in range(1, ins.shape[1]):\n",
    "                x_i = ins[i_t, i_lab]\n",
    "                if not torch.isnan(x_i).item():\n",
    "                    # Normalize the value\n",
    "                    norm_value = (x_i - self.means[i_lab]) / (self.stds[i_lab] + 1e-7)\n",
    "                    x_ts[bin_idx, i_lab-1] = norm_value\n",
    "                    # Mark this value as observed\n",
    "                    x_ts[bin_idx, i_lab-1+self.d_time_series_num()] += 1\n",
    "        \n",
    "        # Calculate bin end times\n",
    "        bin_ends = torch.arange(1, self.n_timesteps+1) / self.n_timesteps * max_days\n",
    "        \n",
    "        # Package the data\n",
    "        x = (x_ts, x_static, bin_ends)\n",
    "        y = self.y[i]\n",
    "        \n",
    "        # Cache if needed\n",
    "        if self.temp_cache is not None:\n",
    "            self.temp_cache[i] = (x, y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def d_static_num(self):\n",
    "        \"\"\"The total dimension of numeric static features\"\"\"\n",
    "        return 0  # No static features in this implementation\n",
    "    \n",
    "    def d_time_series_num(self):\n",
    "        \"\"\"The total dimension of numeric time-series features\"\"\"\n",
    "        return len(self.lab_columns)\n",
    "    \n",
    "    def d_target(self):\n",
    "        return 1\n",
    "    \n",
    "    def pos_frac(self):\n",
    "        if self.y is not None:\n",
    "            return self.y.float().mean().item()\n",
    "        return 0.5\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_into_seqs(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    return zip(*xs), torch.tensor(ys)\n",
    "\n",
    "class DialysisDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path=None, use_temp_cache=False, batch_size=8, \n",
    "                 num_workers=1, prefetch_factor=2, target_col='outcome', \n",
    "                 seed=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.use_temp_cache = use_temp_cache\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.target_col = target_col\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Create datasets for each split\n",
    "        self.ds_train = DialysisDataset('train', data_path=self.data_path, \n",
    "                                        use_temp_cache=use_temp_cache,\n",
    "                                        target_col=target_col, seed=seed)\n",
    "        self.ds_val = DialysisDataset('val', data_path=self.data_path, \n",
    "                                     use_temp_cache=use_temp_cache,\n",
    "                                     target_col=target_col, seed=seed)\n",
    "        self.ds_test = DialysisDataset('test', data_path=self.data_path, \n",
    "                                      use_temp_cache=use_temp_cache,\n",
    "                                      target_col=target_col, seed=seed)\n",
    "\n",
    "        self.prepare_data_per_node = False\n",
    "        \n",
    "        # DataLoader arguments\n",
    "        self.dl_args = {'batch_size': self.batch_size, 'prefetch_factor': self.prefetch_factor,\n",
    "                'collate_fn': collate_into_seqs, 'num_workers': num_workers}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage is None:\n",
    "            self.ds_train.setup()\n",
    "            self.ds_val.setup()\n",
    "            self.ds_test.setup()\n",
    "        elif stage == 'fit':\n",
    "            self.ds_train.setup()\n",
    "            self.ds_val.setup()\n",
    "        elif stage == 'validate':\n",
    "            self.ds_val.setup()\n",
    "        elif stage == 'test':\n",
    "            self.ds_test.setup()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, shuffle=True, **self.dl_args)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, **self.dl_args)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, **self.dl_args)\n",
    "    \n",
    "    def d_static_num(self):\n",
    "        return self.ds_train.d_static_num()\n",
    "\n",
    "    def d_time_series_num(self):\n",
    "        return self.ds_train.d_time_series_num()\n",
    "\n",
    "    def d_target(self):\n",
    "        return self.ds_train.d_target()\n",
    "\n",
    "    def pos_frac(self):\n",
    "        return self.ds_train.pos_frac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
